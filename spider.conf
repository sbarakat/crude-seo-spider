#
# Crude SEO Spider Config File
#

# The url to start spidering from
base_url = http://example.com/

# Only crawl links that blong to these hosts
same_hosts = www.example.com,example.com,blog.example.com,forum.example.com

# The user agent and email address used by the spider
agent = spider http://example.com/
email = spider@domain.invalid

# List of HTML elements to extract links from
#link_tags = a,frame,form,link,img,script
link_tags = a,frame,form,link

# Time in minutes to spend crawling the site, 0 to disable
max_time = 0

# The maximum number of links to crawl, 0 to disable
max_indexed = 0

# Delay in seconds between each link crawled, ignored if keep_alive is set to 1
delay_sec = 10

# Use the same connection for subsequent requests
keep_alive = 1

# Check for duplicate content using MD5 hashes
use_md5 = 1

# Ignore the robots.txt file
ignore_robots_file = 0

# Ignore the rel=nofollow attribute in anchor tags
ignore_nofollow = 0

# Not implemented yet, but should be greater than 1
max_depth = 20
